{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7635efc2-d906-42f1-aa0c-3341c3d3c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25fd46d7-3dfd-437c-b996-14401026b323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01665119-effc-4d77-9b16-3eaf7198b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Truth_Seeker_Model_Dataset.csv'\n",
    "file_path2 = 'Features_For_Traditional_ML_Techniques.csv'\n",
    "file_path3 = 'Downloads/Truth_Seeker_Model_Dataset_With_TimeStamps 1.xlsx'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "df3 = pd.read_excel(file_path3)\n",
    "\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "df2 = df2.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "df3 = df3.drop(columns=[\"Unnamed: 0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06f1b212-5a11-4de7-81c2-6d24f887d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7c5ed5e-dcd7-4f61-b187-8c66ef931878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83855f17-2df8-49b4-ab63-f76872ebdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import *  # this is HuggingFace library\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ba8989b-3f0f-4bbb-817f-2e416a3e2d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\T2330114\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\T2330114\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\T2330114\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\T2330114\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\T2330114\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\T2330114\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Loaded 109,482,240 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this will download the BERT Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  \n",
    "# this will download the BERT Trained Model\n",
    "# output_hidden_states=False, as we are training & not interested in output state.\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\",output_hidden_states=False) # dropout=0.2, attention_dropout=0.2\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "747afbc8-f8cd-4e42-a1ed-2bec6d55a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def clean_text(temp):\n",
    "    temp=re.sub(\"@\\S+\", \" \", temp)\n",
    "    temp=re.sub(\"https*\\S+\", \" \", temp)\n",
    "    temp=re.sub(\"#\\S+\", \" \", temp)\n",
    "    temp=re.sub(\"\\'\\w+\", '', temp)\n",
    "    temp=re.sub(r'\\w*\\d+\\w*', '', temp)\n",
    "    temp=re.sub('\\s{2,}', \" \", temp)\n",
    "   \n",
    "    return temp.strip()\n",
    "\n",
    "df['tweet_clean'] = df['tweet'].apply(clean_text)\n",
    "df['stat'] = df['statement'].apply(clean_text)\n",
    "df['auth'] = df['author'].apply(clean_text)\n",
    "df['mank'] = df['manual_keywords'].apply(clean_text)\n",
    "df['fivlab'] = df['5_label_majority_answer'].apply(clean_text)\n",
    "df['thrlab'] = df['3_label_majority_answer'].apply(clean_text)\n",
    "#sentences = df['tweet_clean'] + \" \"+ df['stat'] + \" \" + df['auth'] + \" \" + df['mank'] + \" \" + df['fivlab'] + \" \" + df['thrlab'] \n",
    "\n",
    "sentences = df['tweet_clean'] + \" \"+ df['stat'] + \" \" + df['auth'] + \" \" + df['mank']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a715471-0e42-481f-b461-bb31f0359827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 134198/134198 [03:12<00:00, 697.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_ids=[]\n",
    "attention_masks=[]\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,truncation=True,max_length =28,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids.append(bert_inp['input_ids'])\n",
    "    attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids=np.asarray(input_ids)\n",
    "attention_masks=np.array(attention_masks)\n",
    "target = np.array(df['BinaryNumTarget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66d12058-52a7-4b04-8f16-6df32e492e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bert_tokenizer.convert_ids_to_tokens(101)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c800e9f-0a6d-4b81-bc5b-340c242f463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test,train_mask,test_mask=train_test_split(input_ids,target,attention_masks,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8356c5b4-01a3-4d0b-8dfb-8435cb96ea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c65a6c5-a4f9-4412-bc09-d9205f7fa973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_token (InputLayer)       [(None, 28)]         0           []                               \n",
      "                                                                                                  \n",
      " masked_token (InputLayer)      [(None, 28)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_token[0][0]',            \n",
      "                                thPoolingAndCrossAt               'masked_token[0][0]']           \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 28,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 28, 16)      49728       ['tf_bert_model_1[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 16)          0           ['bidirectional_1[0][0]']        \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            136         ['global_max_pooling1d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)           (None, 8)            0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            9           ['dropout_75[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,532,113\n",
      "Trainable params: 49,873\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM, Dense, Dropout, Flatten, Concatenate, GlobalMaxPool1D , Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Textual Input\n",
    "# text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "# embedding = Embedding(input_dim=max_num_words, output_dim=embedding_dim, input_length=max_sequence_length)(text_input)\n",
    "input_ids_in = tf.keras.layers.Input(shape=(28,), name='input_token', dtype='int32')\n",
    "input_masks_in = tf.keras.layers.Input(shape=(28,), name='masked_token', dtype='int32') \n",
    "embedding_layer = bert_model(input_ids_in, attention_mask=input_masks_in)[0] \n",
    "lstm = Bidirectional(LSTM(8, return_sequences=True, dropout=0.1, recurrent_dropout=0))(embedding_layer)\n",
    "pooling = GlobalMaxPool1D()(lstm)\n",
    "X = Dense(8, activation='relu')(pooling)\n",
    "X = Dropout(0.2)(X)\n",
    "X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "model = Model(inputs=[input_ids_in, input_masks_in], outputs=X)\n",
    "\n",
    "\n",
    "for layer in bert_model.layers[:3]:  \n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c9b819a-410a-4d8f-bfab-5df238e14ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = \"finaltraining_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcd290b5-4a3d-47d4-be70-ef954308540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "3355/3355 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8271\n",
      "Epoch 1: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 217s 61ms/step - loss: 0.3798 - accuracy: 0.8271 - val_loss: 0.2495 - val_accuracy: 0.9007\n",
      "Epoch 2/8\n",
      "3354/3355 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.8937\n",
      "Epoch 2: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 205s 61ms/step - loss: 0.2751 - accuracy: 0.8937 - val_loss: 0.2284 - val_accuracy: 0.9058\n",
      "Epoch 3/8\n",
      "3355/3355 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9060\n",
      "Epoch 3: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 201s 60ms/step - loss: 0.2425 - accuracy: 0.9060 - val_loss: 0.1967 - val_accuracy: 0.9190\n",
      "Epoch 4/8\n",
      "3354/3355 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9118\n",
      "Epoch 4: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 203s 61ms/step - loss: 0.2245 - accuracy: 0.9118 - val_loss: 0.1917 - val_accuracy: 0.9238\n",
      "Epoch 5/8\n",
      "3355/3355 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9160\n",
      "Epoch 5: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 204s 61ms/step - loss: 0.2151 - accuracy: 0.9160 - val_loss: 0.1922 - val_accuracy: 0.9224\n",
      "Epoch 6/8\n",
      "3354/3355 [============================>.] - ETA: 0s - loss: 0.2072 - accuracy: 0.9194\n",
      "Epoch 6: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 203s 60ms/step - loss: 0.2072 - accuracy: 0.9194 - val_loss: 0.1855 - val_accuracy: 0.9250\n",
      "Epoch 7/8\n",
      "3355/3355 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9218\n",
      "Epoch 7: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 203s 60ms/step - loss: 0.1990 - accuracy: 0.9218 - val_loss: 0.1877 - val_accuracy: 0.9243\n",
      "Epoch 8/8\n",
      "3355/3355 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9218\n",
      "Epoch 8: saving model to finaltraining_1\\cp.ckpt\n",
      "3355/3355 [==============================] - 204s 61ms/step - loss: 0.1965 - accuracy: 0.9218 - val_loss: 0.1799 - val_accuracy: 0.9281\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history=model.fit([X_train,train_mask],y_train,batch_size=32,epochs=8,validation_data=([X_test,test_mask],y_test),callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40c00b75-7b3b-4cd0-a65a-24fcb8c16869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'checkpoint',\n",
       " 'cp.ckpt.data-00000-of-00001',\n",
       " 'cp.ckpt.index']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "208d4f24-8a22-4caf-863a-86f7afa759d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1feb1925d80>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loads the weights\n",
    "model.load_weights(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8d64946-de64-4bbf-b84c-44150fd8aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839/839 - 32s - loss: 0.1799 - accuracy: 0.9281 - 32s/epoch - 38ms/step\n",
      "Restored model, accuracy: 92.81%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([X_test,test_mask],y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a92bc42e-9920-4313-8413-0d3a7dd2c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839/839 [==============================] - 38s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob = model.predict([X_test,test_mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac598b15-15e6-420e-a0b8-5108e8acd1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9281296572280179\n",
      "Test Precision: 0.9250485996112031\n",
      "Test Recall: 0.9353523587652883\n",
      "F1-score: 0.9301719457013574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca2ffcaf-2faf-44f6-8a9b-41ad504b1028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 573.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "predict_sent = sentences[:20]\n",
    "\n",
    "input_ids2=[]\n",
    "attention_masks2=[]\n",
    "\n",
    "\n",
    "for sent in tqdm(predict_sent):\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,truncation=True,max_length =28,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids2.append(bert_inp['input_ids'])\n",
    "    attention_masks2.append(bert_inp['attention_mask'])\n",
    "\n",
    "input_ids_predict=np.asarray(input_ids2)\n",
    "attention_masks_predict=np.array(attention_masks2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e381ede8-c1c8-4edf-b1fd-0b7897b724b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  label  \n",
       "0  December 31, 2017    0.0  \n",
       "1  December 31, 2017    0.0  \n",
       "2  December 30, 2017    0.0  \n",
       "3  December 29, 2017    0.0  \n",
       "4  December 25, 2017    0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preddf1 = pd.read_csv('ISOT_Fake.csv') ## Fake\n",
    "\n",
    "preddf1['label'] = 0.0\n",
    "\n",
    "preddf2 = pd.read_csv('ISOT_True.csv') ##Real\n",
    "\n",
    "preddf2['label'] = 1.0\n",
    "\n",
    "preddf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eda11376-c023-4905-9baf-48c9b15f7778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  label  \n",
       "0  December 31, 2017     1.0  \n",
       "1  December 29, 2017     1.0  \n",
       "2  December 31, 2017     1.0  \n",
       "3  December 30, 2017     1.0  \n",
       "4  December 29, 2017     1.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "628d0c5e-3c85-4f56-856a-23faba97e768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committ...</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable pr...</td>\n",
       "      <td>US_News</td>\n",
       "      <td>February 13, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Securit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 5, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shi...</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rosse...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>September 27, 2017</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Le...</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed...</td>\n",
       "      <td>News</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a go...</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presid...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>June 24, 2016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ben Stein Calls Out 9th Circuit Court: Committ...   \n",
       "1  Trump drops Steve Bannon from National Securit...   \n",
       "2  Puerto Rico expects U.S. to lift Jones Act shi...   \n",
       "3   OOPS: Trump Just Accidentally Confirmed He Le...   \n",
       "4  Donald Trump heads for Scotland to reopen a go...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  21st Century Wire says Ben Stein, reputable pr...       US_News   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald T...  politicsNews   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rosse...  politicsNews   \n",
       "3  On Monday, Donald Trump once again embarrassed...          News   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presid...  politicsNews   \n",
       "\n",
       "                  date  label  \n",
       "0    February 13, 2017    0.0  \n",
       "1       April 5, 2017     1.0  \n",
       "2  September 27, 2017     1.0  \n",
       "3         May 22, 2017    0.0  \n",
       "4       June 24, 2016     1.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "combine = pd.concat([preddf1, preddf2], ignore_index=True)\n",
    "\n",
    "combine = combine.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3507f689-5863-4a22-beb1-1cd3477c467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(temp):\n",
    "    temp=re.sub(\"@\\S+\", \" \", temp)\n",
    "    temp=re.sub(\"https*\\S+\", \" \", temp)\n",
    "    temp=re.sub(\"#\\S+\", \" \", temp)\n",
    "    temp=re.sub(\"\\'\\w+\", '', temp)\n",
    "    temp=re.sub(r'\\w*\\d+\\w*', '', temp)\n",
    "    temp=re.sub('\\s{2,}', \" \", temp)\n",
    "    return temp.strip()\n",
    "\n",
    "\n",
    "combine['tile'] = combine['title'].apply(clean_text)\n",
    "combine['text_clean'] = combine['text'].apply(clean_text)\n",
    "combine['sub'] = combine['subject'].apply(clean_text)\n",
    "combine['dat'] = combine['date'].apply(clean_text)\n",
    "\n",
    "\n",
    "#sentences3 = combine['tile'] + \" \" + combine['text_clean'] + \" \" + combine['sub'] + \" \" + combine['dat']\n",
    "sentences3 = combine['tile'] + \" \" + combine['text_clean'] + \" \" + combine['sub']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a7e6955-b92c-49f0-a8a1-29beebed8022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 44898/44898 [07:35<00:00, 98.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids3=[]\n",
    "attention_masks3=[]\n",
    "\n",
    "\n",
    "for sent in tqdm(sentences3):\n",
    "    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,truncation=True,max_length =28,pad_to_max_length = True,return_attention_mask = True)\n",
    "    input_ids3.append(bert_inp['input_ids'])\n",
    "    attention_masks3.append(bert_inp['attention_mask'])\n",
    "\n",
    "new_input_ids=np.asarray(input_ids3)\n",
    "new_attention_mask=np.array(attention_masks3)\n",
    "new_labels = np.array(combine['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47502126-6f0f-4734-82b2-dd8cec1b99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_trainnew, X_testnew, y_trainnew, y_testnew,train_masknew,test_masknew=train_test_split(new_input_ids,new_labels,new_attention_mask,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a51090bd-c58e-4e65-9b2a-16af497465ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path2 = \"finaltraining_2/cp.ckpt\"\n",
    "checkpoint_dir2 = os.path.dirname(checkpoint_path2)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback2 = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path2,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Recompile the model with a smaller learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),  # Smaller learning rate for fine-tuning\n",
    "    loss='binary_crossentropy',         # Same loss function as before\n",
    "    metrics=['accuracy']                # Same evaluation metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07ee3109-b7a6-42fc-ad90-c0694525875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1122/1123 [============================>.] - ETA: 0s - loss: 0.8376 - accuracy: 0.6241\n",
      "Epoch 1: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 85s 64ms/step - loss: 0.8375 - accuracy: 0.6241 - val_loss: 0.6144 - val_accuracy: 0.7145\n",
      "Epoch 2/7\n",
      "1123/1123 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.7718\n",
      "Epoch 2: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 70s 62ms/step - loss: 0.5101 - accuracy: 0.7718 - val_loss: 0.4062 - val_accuracy: 0.8274\n",
      "Epoch 3/7\n",
      "1123/1123 [==============================] - ETA: 0s - loss: 0.3416 - accuracy: 0.8614\n",
      "Epoch 3: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 68s 60ms/step - loss: 0.3416 - accuracy: 0.8614 - val_loss: 0.2304 - val_accuracy: 0.9117\n",
      "Epoch 4/7\n",
      "1122/1123 [============================>.] - ETA: 0s - loss: 0.2053 - accuracy: 0.9268\n",
      "Epoch 4: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 69s 62ms/step - loss: 0.2053 - accuracy: 0.9268 - val_loss: 0.1296 - val_accuracy: 0.9550\n",
      "Epoch 5/7\n",
      "1123/1123 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9583\n",
      "Epoch 5: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 69s 61ms/step - loss: 0.1289 - accuracy: 0.9583 - val_loss: 0.0855 - val_accuracy: 0.9686\n",
      "Epoch 6/7\n",
      "1123/1123 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9722\n",
      "Epoch 6: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 68s 60ms/step - loss: 0.0934 - accuracy: 0.9722 - val_loss: 0.0636 - val_accuracy: 0.9766\n",
      "Epoch 7/7\n",
      "1123/1123 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9793\n",
      "Epoch 7: saving model to finaltraining_2\\cp.ckpt\n",
      "1123/1123 [==============================] - 69s 61ms/step - loss: 0.0697 - accuracy: 0.9793 - val_loss: 0.0507 - val_accuracy: 0.9821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fine-tuning with new data\n",
    "history2 = model.fit(\n",
    "    [X_trainnew, train_masknew],  # New input data\n",
    "     y_trainnew,                           # New labels\n",
    "    batch_size=32,\n",
    "    epochs=7,  # Use fewer epochs for fine-tuning\n",
    "    validation_data=([X_testnew, test_masknew], y_testnew),callbacks=[cp_callback2] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be60ea7d-bfc6-4a28-84fc-157bd8fbf3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x20037a7d780>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loads the weights\n",
    "model.load_weights(checkpoint_path2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20731177-623b-42bb-97f6-a19f1f69fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 - 11s - loss: 0.0507 - accuracy: 0.9821 - 11s/epoch - 39ms/step\n",
      "Restored model, accuracy: 98.21%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss, acc = model.evaluate([X_testnew, test_masknew], y_testnew, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3454a31-be66-4b9d-ae04-900e4459b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 15s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob2 = model.predict([X_testnew, test_masknew])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7831da5-08af-4b47-848a-a45eee53e878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9820712694877506\n",
      "Test Precision: 0.9745899745899745\n",
      "Test Recall: 0.988056206088993\n",
      "F1-score: 0.9812768926619374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred2 = (y_pred_prob2 > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "accuracy2 = accuracy_score(y_testnew, y_pred2)\n",
    "precision2 = precision_score(y_testnew, y_pred2)\n",
    "recall2 = recall_score(y_testnew, y_pred2)\n",
    "f12 = f1_score(y_testnew, y_pred2)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy2}\")\n",
    "print(f\"Test Precision: {precision2}\")\n",
    "print(f\"Test Recall: {recall2}\")\n",
    "print(f\"F1-score: {f12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c2392-d95a-486d-a614-d6e96ea125c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d428358-100c-426a-8a4f-c585fe6ad66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
